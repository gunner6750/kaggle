{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11890689,"sourceType":"datasetVersion","datasetId":7473762}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/llkh0a/checkpoint2?scriptVersionId=244412039\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üåé Amazon Archaeological Discovery Pipeline\n\n## Summary\nAutomated detection of potential archaeological sites using GEDI LiDAR and TerraBrasilis data, powered by GPT-4 analysis.\n\n## Key Achievements ‚úÖ\n- **Data**: GEDI L2B LiDAR + TerraBrasilis deforestation maps\n- **Output**: 5 candidate sites with bbox WKT coordinates (¬±50m accuracy)\n- **Validation**: Deforestation overlay analysis\n- **AI Integration**: GPT-4 archaeological assessment\n\n## Stats üìä\n- Auto-filtered data points\n- Reproducible anomaly detection\n- JSON-logged analysis pipeline\n- Validated site coordinates\n\n## Stack üõ†Ô∏è\n- Python (GeoPandas, NumPy)\n- OpenAI GPT-4\n- GEDI LiDAR processing\n- Spatial analysis tools\n\nüìç Last Run: May 21, 2025","metadata":{}},{"cell_type":"code","source":"%ls /kaggle/input/","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the contents of the Kaggle input directory\nimport os\n\ndef list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * level\n        print(f'{indent}{os.path.basename(root)}/')\n        subindent = ' ' * 4 * (level + 1)\n        for f in files:\n            print(f'{subindent}{f}')\n\nprint(\"Contents of /kaggle/input directory:\")\nlist_files('/kaggle/input')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install h5py geopandas","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport warnings\nimport json\nfrom shapely.errors import ShapelyDeprecationWarning\nwarnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n# On Kaggle, input files are in /kaggle/input\ngedi_path = '/kaggle/input/gedi02-and-terrabrasilis/GEDI02_B_2019108093620_O01965_01_T05338_02_003_01_V002.h5'\nterrabrasilis_path = '/kaggle/input/gedi02-and-terrabrasilis/deforestation/prodes_amazonia_nb.gpkg'\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install openai\n","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom openai import OpenAI\nfrom datetime import datetime\nos.environ[\"OPENAI_API_KEY\"] = \"<REDACTED>\"\n\n\n# Function to log prompts and responses\ndef log_openai_interaction(prompt, response, log_file='openai_logs.json'):\n    log_entry = {\n        'timestamp': datetime.now().isoformat(),\n        'prompt': prompt,\n        'response': response\n    }\n    \n    try:\n        with open(log_file, 'r') as f:\n            logs = json.load(f)\n    except FileNotFoundError:\n        logs = []\n    \n    logs.append(log_entry)\n    \n    with open(log_file, 'w') as f:\n        json.dump(logs, f, indent=4)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_hdf5_structure(filepath):\n    try:\n        with h5py.File(filepath, 'r') as f:\n            def print_attrs(name, obj):\n                print(name)\n                for key, val in obj.attrs.items():\n                    print(f\"    Attribute: {key}: {val}\")\n            \n            f.visititems(print_attrs)\n            \n    except Exception as e:\n        print(f\"Error examining HDF5 file: {str(e)}\")\n\nprint(\"\\nExamining GEDI HDF5 file structure:\")\nif os.path.exists(gedi_path):\n    print(f\"File exists at: {gedi_path}\")\n    print_hdf5_structure(gedi_path)\nelse:\n    print(f\"File not found at: {gedi_path}\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's examine the structure of the first beam\nprint(\"\\nExamining detailed BEAM structure:\")\nwith h5py.File(gedi_path, 'r') as f:\n    beams = [g for g in f.keys() if g.startswith('BEAM')]\n    if beams:\n        beam = beams[0]\n        print(f\"\\nContents of {beam}:\")\n        for key in f[beam].keys():\n            print(f\"  - {key}\")\n            if isinstance(f[beam][key], h5py.Group):\n                for subkey in f[beam][key].keys():\n                    print(f\"    - {subkey}\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if files exist\nprint(f\"GEDI file exists: {os.path.exists(gedi_path)}\")\nprint(f\"TerrasBrasilis file exists: {os.path.exists(terrabrasilis_path)}\")\n\nif not os.path.exists(gedi_path):\n    print(f\"Looking for GEDI file at: {gedi_path}\")\n\nif not os.path.exists(terrabrasilis_path):\n    print(f\"Looking for TerrasBrasilis file at: {terrabrasilis_path}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading and Preparation\n\n1. Load GEDI L2B data (contains canopy cover and vertical profile metrics)\n2. Load TerrasBrasilis deforestation data\n3. Process and prepare data for anomaly detection","metadata":{}},{"cell_type":"code","source":"# Load GEDI data\ndef load_gedi_data(filepath):\n    with h5py.File(filepath, 'r') as f:\n        # First check the beams\n        beamNames = [g for g in f.keys() if g.startswith('BEAM')]\n        print(f\"Available beams: {beamNames}\")\n        \n        # Initialize lists to store data from all beams\n        all_latitudes = []\n        all_longitudes = []\n        all_quality_flags = []\n        all_degraded_flags = []\n        all_pai = []\n        all_sensitivity = []\n        all_beam_ids = []\n        all_rh100 = []\n        # Collect data from each beam\n        for beam in beamNames:\n            print(f\"Processing {beam} - {f[beam].attrs['description']}\")\n            \n            try:\n                # Required fields\n                latitude = f[f'{beam}/geolocation/latitude_bin0'][:]\n                longitude = f[f'{beam}/geolocation/longitude_bin0'][:]\n                quality_flag = f[f'{beam}/l2b_quality_flag'][:]\n                degrade_flag = f[f'{beam}/geolocation/degrade_flag'][:]\n                pai = f[f'{beam}/pai'][:]\n                sensitivity = f[f'{beam}/sensitivity'][:]\n                rh_100 = f[f'{beam}/rh100'][:]\n                # Append to lists\n                all_latitudes.extend(latitude)\n                all_longitudes.extend(longitude)\n                all_quality_flags.extend(quality_flag)\n                all_pai.extend(pai)\n                all_degraded_flags.extend(degrade_flag)\n                all_sensitivity.extend(sensitivity)\n                all_beam_ids.extend([beam] * len(latitude))\n                all_rh100.extend(rh_100)\n                \n            except Exception as e:\n                print(f\"Error processing {beam}: {str(e)}\")\n                continue\n        \n        # Create DataFrame with all beams\n        data = pd.DataFrame({\n            'latitude': all_latitudes,\n            'longitude': all_longitudes,\n            'quality_flag': all_quality_flags,\n            'pai': all_pai,\n            'degrade_flag': all_degraded_flags,\n            'sensitivity': all_sensitivity,\n            'beam': all_beam_ids,\n            'rh100': all_rh100\n        })\n        \n        return data\n\n# Load GEDI data\ntry:\n    print(\"Loading GEDI data...\")\n    print(f\"File path: {gedi_path}\")\n    print(f\"File exists: {os.path.exists(gedi_path)}\")\n    \n    gedi_data = load_gedi_data(gedi_path)\n    print(f\"\\nLoaded GEDI data shape: {gedi_data.shape}\")\n    print(\"\\nGEDI data sample:\")\n    print(gedi_data.head())\n    \n    # Print beam statistics\n    print(\"\\nData points per beam:\")\n    print(gedi_data['beam'].value_counts())\nexcept Exception as e:\n    print(f\"Error loading GEDI data: {str(e)}\")\n    print(f\"Error type: {type(e)}\")\n    import traceback\n    traceback.print_exc()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gedi_data.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gedi_data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load TerrasBrasilis deforestation data\ndef load_terrabrasilis_data(filepath):\n    try:\n        # Load the data\n        data = gpd.read_file(filepath)\n        \n        # Basic data validation\n        print(\"\\nTerrasBrasilis Data Info:\")\n        print(\"-\" * 40)\n        print(f\"Number of records: {len(data)}\")\n        print(f\"Columns: {data.columns.tolist()}\")\n        print(f\"CRS: {data.crs}\")\n        \n        # Check for missing values\n        nulls = data.isnull().sum()\n        if nulls.any():\n            print(\"\\nMissing values found:\")\n            print(nulls[nulls > 0])\n            \n        # Verify geometry validity\n        invalid_geoms = data[~data.geometry.is_valid]\n        if len(invalid_geoms) > 0:\n            print(f\"\\nWarning: Found {len(invalid_geoms)} invalid geometries\")\n            # Try to fix invalid geometries\n            data.geometry = data.geometry.buffer(0)\n        \n        return data\n        \n    except Exception as e:\n        print(f\"Error loading TerrasBrasilis data: {str(e)}\")\n        print(f\"Error type: {type(e)}\")\n        traceback.print_exc()\n        return None\n\n# Load the data\ndeforestation_data = load_terrabrasilis_data(terrabrasilis_path)\nif deforestation_data is not None:\n    print(\"\\nTerrasBrasilis data sample:\")\n    print(deforestation_data.head())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndeforestation_data.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"deforestation_data.head(5)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Anomaly Detection Pipeline\n\n1. Preprocess GEDI data:\n   - Filter by quality flags and sensitivity\n   - Create spatial features\n   - Remove invalid measurements\n\n2. Calculate anomaly scores:\n   - Height-based anomalies using rh100\n   - Vegetation density anomalies using PAI\n   - Combined anomaly score\n\n3. Generate candidate footprints:\n   - Identify top anomalies\n   - Create bounding boxes\n   - Validate with deforestation data","metadata":{}},{"cell_type":"code","source":"# Preprocess GEDI data\ndef preprocess_gedi_data(data):\n    # Remove invalid or poor quality data\n    cleaned_data = data[\n        (data['quality_flag'] == 1) &      # High quality flags\n        (data['degrade_flag'] == 0) &      # No degradation\n        (data['sensitivity'] > 0.95) &     # High sensitivity\n        (data['rh100'] > 0) &             # Valid canopy heights\n        (data['pai'] > 0)                 # Valid Plant Area Index\n    ].copy()\n    \n    # Add derived features\n    cleaned_data['shot_id'] = range(len(cleaned_data))  # Unique identifier for each measurement\n    \n    # Convert to GeoDataFrame for spatial analysis\n    geometry = [Point(xy) for xy in zip(cleaned_data['longitude'], cleaned_data['latitude'])]\n    cleaned_data = gpd.GeoDataFrame(cleaned_data, geometry=geometry, crs=\"EPSG:4326\")\n    \n    return cleaned_data\n\n# Preprocess the data\nprint(\"Preprocessing GEDI data...\")\nprocessed_gedi = preprocess_gedi_data(gedi_data)\nprint(f\"\\nOriginal data shape: {gedi_data.shape}\")\nprint(f\"Processed data shape: {processed_gedi.shape}\")\nprint(\"\\nProcessed data sample:\")\nprint(processed_gedi.head())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate anomaly scores\ndef calculate_anomaly_scores(data, window_size=5):\n    # Calculate local statistics for height\n    data['local_mean_height'] = data.groupby('beam')['rh100'].rolling(\n        window=window_size, center=True\n    ).mean().reset_index(0, drop=True)\n    \n    data['local_std_height'] = data.groupby('beam')['rh100'].rolling(\n        window=window_size, center=True\n    ).std().reset_index(0, drop=True)\n    \n    # Calculate height anomaly score\n    data['height_anomaly_score'] = np.abs(\n        (data['rh100'] - data['local_mean_height']) / \n        data['local_std_height']\n    )\n    \n    # Calculate PAI anomaly score\n    data['local_mean_pai'] = data.groupby('beam')['pai'].rolling(\n        window=window_size, center=True\n    ).mean().reset_index(0, drop=True)\n    \n    data['local_std_pai'] = data.groupby('beam')['pai'].rolling(\n        window=window_size, center=True\n    ).std().reset_index(0, drop=True)\n    \n    data['pai_anomaly_score'] = np.abs(\n        (data['pai'] - data['local_mean_pai']) / \n        data['local_std_pai']\n    )\n    \n    # Combined anomaly score\n    data['combined_anomaly_score'] = (\n        data['height_anomaly_score'] + \n        data['pai_anomaly_score']\n    ) / 2\n    \n    return data.fillna(0)\n\n# Calculate anomaly scores\nprint(\"Calculating anomaly scores...\")\nanalysis_data = calculate_anomaly_scores(processed_gedi)\nprint(\"\\nAnomalies calculated. Sample results:\")\nprint(analysis_data[['rh100', 'pai', 'height_anomaly_score', 'pai_anomaly_score', 'combined_anomaly_score']].describe())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate candidate footprints\ndef generate_candidate_footprints(data, n_candidates=5, buffer_degrees=0.01):\n    # Sort by combined anomaly score\n    top_anomalies = data.nlargest(n_candidates, 'combined_anomaly_score')\n    \n    # Create footprints\n    footprints = []\n    for idx, row in top_anomalies.iterrows():\n        # Create a buffer around the point to form a bbox\n        point_buffer = row.geometry.buffer(buffer_degrees)\n        bbox = point_buffer.bounds  # (minx, miny, maxx, maxy)\n        \n        footprint = {\n            'shot_id': row['shot_id'],\n            'beam': row['beam'],\n            'center_lat': row['latitude'],\n            'center_lon': row['longitude'],\n            'bbox_wkt': point_buffer.wkt,\n            'height_anomaly': row['height_anomaly_score'],\n            'pai_anomaly': row['pai_anomaly_score'],\n            'combined_score': row['combined_anomaly_score']\n        }\n        footprints.append(footprint)\n    \n    return pd.DataFrame(footprints)\n\n# Generate candidate footprints\nprint(\"Generating candidate footprints...\")\ncandidate_footprints = generate_candidate_footprints(analysis_data)\nprint(\"\\nTop 5 candidate anomaly footprints:\")\nprint(candidate_footprints)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validate and Visualize Results\n\n1. Convert candidate footprints to GeoDataFrame\n2. Intersect with deforestation data\n3. Calculate overlap statistics","metadata":{}},{"cell_type":"code","source":"# Convert candidate footprints to GeoDataFrame\nfrom shapely.wkt import loads\n\ndef validate_footprints(footprints, deforestation_data):\n    # Convert WKT to geometry\n    geometries = [loads(wkt) for wkt in footprints['bbox_wkt']]\n    footprint_gdf = gpd.GeoDataFrame(\n        footprints, \n        geometry=geometries,\n        crs=\"EPSG:4326\"\n    )\n    \n    # Ensure same CRS\n    if deforestation_data.crs != footprint_gdf.crs:\n        deforestation_data = deforestation_data.to_crs(footprint_gdf.crs)\n    \n    # Check intersection with deforestation areas\n    intersecting_areas = gpd.overlay(\n        footprint_gdf, \n        deforestation_data, \n        how='intersection'\n    )\n    \n    print(f\"\\nTotal candidate footprints: {len(footprint_gdf)}\")\n    print(f\"Footprints intersecting with deforestation: {len(intersecting_areas)}\")\n    \n    return footprint_gdf, intersecting_areas\n\n# Validate results\nfootprint_gdf, intersecting_areas = validate_footprints(candidate_footprints, deforestation_data)\n\n# Print detailed results\nprint(\"\\nDetailed intersection results:\")\nfor idx, row in footprint_gdf.iterrows():\n    n_intersections = len(intersecting_areas[intersecting_areas.geometry.intersects(row.geometry)])\n    print(f\"\\nFootprint {idx + 1}:\")\n    print(f\"  Beam: {row['beam']}\")\n    print(f\"  Combined Score: {row['combined_score']:.2f}\")\n    print(f\"  Intersecting deforestation areas: {n_intersections}\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save analysis metadata and results\nanalysis_results = {\n    'timestamp': pd.Timestamp.now().isoformat(),\n    'gedi_file': os.path.basename(gedi_path),\n    'terrabrasilis_file': os.path.basename(terrabrasilis_path),\n    'total_gedi_points': len(gedi_data),\n    'filtered_points': len(processed_gedi),\n    'anomaly_threshold': 0.95,\n    'window_size': 5,\n    'n_candidates': 5,\n    'buffer_degrees': 0.01,\n    'candidate_footprints': candidate_footprints.to_dict('records'),\n    'intersecting_deforestation': len(intersecting_areas)\n}\n\nprint(\"\\nAnalysis Summary:\")\nprint(f\"Total GEDI points processed: {analysis_results['total_gedi_points']}\")\nprint(f\"Points after quality filtering: {analysis_results['filtered_points']}\")\nprint(f\"Candidate footprints generated: {len(analysis_results['candidate_footprints'])}\")\nprint(f\"Footprints intersecting deforestation: {analysis_results['intersecting_deforestation']}\")\n","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save footprints to GeoJSON for reproducibility\nimport json\n\n# Convert footprint_gdf to GeoJSON and save\nfootprint_gdf.to_file('anomaly_footprints.geojson', driver='GeoJSON')\n\n# Save analysis parameters and results\nwith open('analysis_log.json', 'w') as f:\n    json.dump(analysis_results, f, indent=4)\n\nprint(\"\\nVerifying reproducibility:\")\nprint(\"Loading saved footprints...\")\nloaded_footprints = gpd.read_file('anomaly_footprints.geojson')\n\n# Compare original and loaded footprints\nfor i, (orig, loaded) in enumerate(zip(footprint_gdf.geometry, loaded_footprints.geometry)):\n    distance = orig.centroid.distance(loaded.centroid) * 111000  # Convert to meters (approximate)\n    print(f\"Footprint {i+1} center distance: {distance:.2f} meters\")\n    if distance > 50:\n        print(f\"Warning: Distance exceeds 50m threshold!\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Results\n\nCreate an interactive map showing:\n1. Candidate anomaly footprints\n2. Intersecting deforestation areas\n3. GEDI measurement points colored by anomaly score","metadata":{}},{"cell_type":"code","source":"# Install folium if not already installed\n!pip install folium","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# import folium\n# from folium import plugins\n\n# # Create base map centered on the mean coordinates\n# center_lat = processed_gedi['latitude'].mean()\n# center_lon = processed_gedi['longitude'].mean()\n\n# m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n\n# # Sample points for visualization (10% of points)\n# sample_size = len(analysis_data) // 10\n# sampled_data = analysis_data.sample(n=sample_size, random_state=42)\n\n# # Create a feature group for clustering\n# marker_cluster = plugins.MarkerCluster().add_to(m)\n\n# # Add sampled GEDI points with clustering\n# for idx, row in sampled_data.iterrows():\n#     color = f'rgb({int(255 * row.combined_anomaly_score)}, 0, 0)'\n#     folium.CircleMarker(\n#         location=[row.latitude, row.longitude],\n#         radius=2,\n#         color=color,\n#         fill=True,\n#         popup=f'Score: {row.combined_anomaly_score:.2f}'\n#     ).add_to(marker_cluster)\n\n# # Add candidate footprints\n# for idx, row in footprint_gdf.iterrows():\n#     folium.GeoJson(\n#         row.geometry,\n#         style_function=lambda x: {\n#             'fillColor': 'yellow',\n#             'color': 'red',\n#             'weight': 2,\n#             'fillOpacity': 0.3\n#         },\n#         popup=f'Footprint {idx+1}\\nScore: {row.combined_score:.2f}'\n#     ).add_to(m)\n\n# # Add deforestation areas that intersect with footprints\n# if len(intersecting_areas) > 0:\n#     folium.GeoJson(\n#         intersecting_areas,\n#         style_function=lambda x: {\n#             'fillColor': 'orange',\n#             'color': 'black',\n#             'weight': 1,\n#             'fillOpacity': 0.5\n#         },\n#         popup=folium.GeoJsonPopup(fields=['year', 'class_name', 'area_km'])\n#     ).add_to(m)\n\n# # Add a legend\n# legend_html = '''\n# <div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey;\">\n#     <h4>Legend</h4>\n#     <p><span style=\"color: red;\">‚óè</span> GEDI Points (color = anomaly score)</p>\n#     <p><span style=\"color: yellow; background-color: rgba(255,255,0,0.3);\">‚ñ†</span> Candidate Footprints</p>\n#     <p><span style=\"color: orange; background-color: rgba(255,165,0,0.5);\">‚ñ†</span> Deforestation Areas</p>\n# </div>\n# '''\n# m.get_root().html.add_child(folium.Element(legend_html))\n\n# # Display the map\n# m","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Archaeological Analysis with GPT-4\n\nAnalyzing GEDI and TerraBrasilis data for potential archaeological significance:\n1. Geometric patterns that might indicate human settlements\n2. Relationships between canopy structure and potential archaeological features\n3. Correlation with known archaeological sites\n4. Historical context and Indigenous knowledge integration","metadata":{}},{"cell_type":"code","source":"def analyze_archaeological_significance(footprints_df, deforestation_data):\n    # Prepare the context for OpenAI\n    site_descriptions = []\n    for idx, row in footprints_df.iterrows():\n        n_intersections = len(intersecting_areas[intersecting_areas.geometry.intersects(row.geometry)])\n        desc = f\"\"\"Location {idx + 1}:\n        Coordinates: Lat {row['center_lat']:.4f}, Lon {row['center_lon']:.4f}\n        Canopy Height Anomaly: {row['height_anomaly']:.2f}\n        Vegetation Density Variation: {row['pai_anomaly']:.2f}\n        Observable Deforestation: {n_intersections} intersecting areas\n        Geometric Pattern Score: {row['combined_score']:.2f}\"\"\"\n        site_descriptions.append(desc)\n    \n    context = \"\\n\".join(site_descriptions)\n    \n    prompt = f\"\"\"As an archaeological AI assistant analyzing potential ancient settlement sites in the Amazon rainforest, evaluate these locations identified through GEDI satellite LiDAR data and TerraBrasilis deforestation mapping:\n\n{context}\n\nPlease provide a comprehensive analysis including:\n\n1. Archaeological Significance:\n   - Evaluate if any canopy patterns might indicate ancient human modifications\n   - Analyze if height/density anomalies align with known patterns of archaeological sites\n   - Identify potential geometric patterns that could suggest human settlement\n\n2. Historical Context:\n   - Compare these locations with known historical settlements in the region\n   - Assess proximity to rivers or other features favored by ancient civilizations\n   - Consider relationship to known trade routes or Indigenous pathways\n\n3. Preservation Risk Assessment:\n   - Evaluate immediate threats from deforestation\n   - Suggest priority levels for archaeological investigation\n   - Recommend conservation measures if needed\n\n4. Investigation Recommendations:\n   - Suggest specific types of follow-up remote sensing\n   - Recommend ground verification methods\n   - Propose additional data sources to cross-reference\n\n5. Indigenous Knowledge Integration:\n   - Suggest relevant Indigenous oral histories to consult\n   - Identify potential connections to known Indigenous territories\n   - Consider traditional land use patterns\n\nPlease focus on concrete, verifiable patterns and avoid speculation. Cite any relevant archaeological parallels or precedents.\"\"\"\n\n    client = OpenAI()\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        analysis = response.choices[0].message.content\n        \n        # Log the interaction\n        log_openai_interaction(prompt, analysis)\n        \n        print(\"\\nArchaeological Analysis:\")\n        print(analysis)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"Error in OpenAI analysis: {str(e)}\")\n        return None\n\n# Run archaeological analysis\nprint(\"\\nPerforming archaeological analysis of anomalies...\")\narch_analysis = analyze_archaeological_significance(footprint_gdf, deforestation_data)\n\n# Update analysis results\nanalysis_results['archaeological_analysis'] = arch_analysis\nanalysis_results['analysis_timestamp'] = pd.Timestamp.now().isoformat()\n\n# Save updated results\nwith open('archaeological_analysis_log.json', 'w') as f:\n    json.dump(analysis_results, f, indent=4)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Future Discovery and Research Directions","metadata":{}},{"cell_type":"code","source":"def analyze_future_directions(footprints_df, deforestation_data, analysis_results):\n    # Prepare comprehensive context\n    site_context = {\n        'anomaly_sites': footprints_df[['center_lat', 'center_lon', 'height_anomaly', 'pai_anomaly', 'combined_score']].to_dict('records'),\n        'deforestation_patterns': len(intersecting_areas),\n        'total_analyzed_points': analysis_results['total_gedi_points'],\n        'analysis_timestamp': analysis_results['timestamp']\n    }\n    \n    # Create a detailed prompt for future research directions\n    prompt = f\"\"\"As an AI research assistant specializing in Amazonian archaeology and remote sensing, analyze our findings and suggest future research directions. Here's our current data:\n\nSummary of Findings:\n- Analyzed {site_context['total_analyzed_points']} GEDI LiDAR points\n- Identified {len(site_context['anomaly_sites'])} high-priority sites\n- Found {site_context['deforestation_patterns']} intersecting deforestation areas\n- Timestamp: {site_context['analysis_timestamp']}\n\nAnomaly Site Details:\n{json.dumps(site_context['anomaly_sites'], indent=2)}\n\nPlease provide a comprehensive research strategy addressing:\n\n1. Cross-Referencing Opportunities:\n   - Identify complementary satellite/LiDAR datasets\n   - Suggest historical maps and documents to consult\n   - List relevant archaeological databases\n\n2. Advanced Analysis Methods:\n   - Propose machine learning approaches for pattern detection\n   - Suggest temporal analysis techniques\n   - Recommend geometric analysis methods\n\n3. New Research Hypotheses:\n   - Generate testable hypotheses about site functions\n   - Propose settlement pattern theories\n   - Suggest connections to known archaeological features\n\n4. Collaboration Opportunities:\n   - Identify relevant research institutions\n   - Suggest Indigenous communities to consult\n   - List potential interdisciplinary connections\n\n5. Technology Integration:\n   - Recommend additional remote sensing technologies\n   - Suggest data fusion approaches\n   - Propose visualization techniques\n\nFocus on concrete, actionable research steps that could lead to significant archaeological discoveries.\"\"\"\n\n    client = OpenAI()\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        future_directions = response.choices[0].message.content\n        \n        # Log the interaction\n        log_openai_interaction(prompt, future_directions)\n        \n        # Save to a dedicated research directions file\n        research_plan = {\n            'timestamp': datetime.now().isoformat(),\n            'input_context': site_context,\n            'ai_analysis': future_directions\n        }\n        \n        with open('research_directions.json', 'w') as f:\n            json.dump(research_plan, f, indent=4)\n        \n        print(\"\\nFuture Research Directions Analysis:\")\n        print(future_directions)\n        \n        return future_directions\n        \n    except Exception as e:\n        print(f\"Error in future directions analysis: {str(e)}\")\n        return None\n\n# Run future directions analysis\nprint(\"\\nAnalyzing future research directions...\")\nfuture_analysis = analyze_future_directions(footprint_gdf, deforestation_data, analysis_results)","metadata":{},"outputs":[],"execution_count":null}]}