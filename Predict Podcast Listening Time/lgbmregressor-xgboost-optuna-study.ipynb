{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/llkh0a/lgbmregressor-xgboost-optuna-study?scriptVersionId=244410804\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# note","metadata":{}},{"cell_type":"markdown","source":"i only use lgb for the older version with\npublic score: 13.05603","metadata":{}},{"cell_type":"markdown","source":"the lastest version has included xgboost, you might copy my parameters instead of do optuna training from the beggining","metadata":{}},{"cell_type":"markdown","source":"# Kaggle Playground Series S5E4 Analysis\n\nLet's analyze the dataset and prepare for model building.","metadata":{}},{"cell_type":"markdown","source":"## Install and Import Required Libraries\nFirst, let's install and import the necessary libraries for our analysis.","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install kaggle pandas numpy matplotlib seaborn plotly","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:21.053901Z","iopub.status.busy":"2025-04-23T08:30:21.053668Z","iopub.status.idle":"2025-04-23T08:30:25.091998Z","shell.execute_reply":"2025-04-23T08:30:25.091348Z","shell.execute_reply.started":"2025-04-23T08:30:21.053883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Set style for plots\nplt.style.use('seaborn')\nsns.set_palette('husl')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:31.935434Z","iopub.status.busy":"2025-04-23T08:30:31.934667Z","iopub.status.idle":"2025-04-23T08:30:33.788826Z","shell.execute_reply":"2025-04-23T08:30:33.788151Z","shell.execute_reply.started":"2025-04-23T08:30:31.935403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load CSV Files into Pandas DataFrames\nRead the downloaded CSV files into pandas DataFrames.","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/input/playground-series-s5e4/\n","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:41.18579Z","iopub.status.busy":"2025-04-23T08:30:41.184969Z","iopub.status.idle":"2025-04-23T08:30:41.190958Z","shell.execute_reply":"2025-04-23T08:30:41.190414Z","shell.execute_reply.started":"2025-04-23T08:30:41.185766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Error handling for file operations\ntry:\n    train_df = pd.read_csv('train.csv')\n    test_df = pd.read_csv('test.csv')\n    print('Training set shape:', train_df.shape)\n    print('Test set shape:', test_df.shape)\nexcept FileNotFoundError as e:\n    print(f'Error: {e}')\n    print('Please ensure the CSV files are in the correct directory.')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:43.018081Z","iopub.status.busy":"2025-04-23T08:30:43.01756Z","iopub.status.idle":"2025-04-23T08:30:45.236725Z","shell.execute_reply":"2025-04-23T08:30:45.235897Z","shell.execute_reply.started":"2025-04-23T08:30:43.018048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initial Data Exploration","metadata":{}},{"cell_type":"code","source":"# Display basic information about the training dataset\nprint(\"\\nTraining Dataset Info:\")\ntrain_df.info()\n\nprint(\"\\nFirst few rows of training data:\")\ntrain_df.head()","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:45.878281Z","iopub.status.busy":"2025-04-23T08:30:45.877611Z","iopub.status.idle":"2025-04-23T08:30:46.128243Z","shell.execute_reply":"2025-04-23T08:30:46.127619Z","shell.execute_reply.started":"2025-04-23T08:30:45.878255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic statistical description\nprint(\"\\nStatistical Description of Training Data:\")\ntrain_df.describe()","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:50.400841Z","iopub.status.busy":"2025-04-23T08:30:50.400406Z","iopub.status.idle":"2025-04-23T08:30:50.601582Z","shell.execute_reply":"2025-04-23T08:30:50.60082Z","shell.execute_reply.started":"2025-04-23T08:30:50.400819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(\"\\nMissing Values in Training Data:\")\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:53.017179Z","iopub.status.busy":"2025-04-23T08:30:53.016504Z","iopub.status.idle":"2025-04-23T08:30:53.232358Z","shell.execute_reply":"2025-04-23T08:30:53.231787Z","shell.execute_reply.started":"2025-04-23T08:30:53.017138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Analysis and Visualization","metadata":{}},{"cell_type":"code","source":"# Function to plot numerical features distribution\ndef plot_numerical_distributions(df, numerical_cols):\n    n_cols = 2\n    n_rows = (len(numerical_cols) + 1) // 2\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n    axes = axes.flatten()\n    \n    for idx, col in enumerate(numerical_cols):\n        sns.histplot(data=df, x=col, ax=axes[idx], kde=True)\n        axes[idx].set_title(f'Distribution of {col}')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:30:55.367336Z","iopub.status.busy":"2025-04-23T08:30:55.366675Z","iopub.status.idle":"2025-04-23T08:30:55.371808Z","shell.execute_reply":"2025-04-23T08:30:55.371081Z","shell.execute_reply.started":"2025-04-23T08:30:55.367311Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df=pd.read_csv('train.csv')\ntest_df=pd.read_csv('test.csv')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:38:48.57995Z","iopub.status.busy":"2025-04-23T08:38:48.579503Z","iopub.status.idle":"2025-04-23T08:38:49.837176Z","shell.execute_reply":"2025-04-23T08:38:49.836585Z","shell.execute_reply.started":"2025-04-23T08:38:48.579929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering\n\nLet's prepare our features with:\n1. Drop unnecessary columns\n2. One-hot encode categorical features\n3. Create binary features for popular hosts/guests\n4. Handle missing values","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering Rationale\n1. **Drop Unnecessary Columns**: Columns like 'Podcast_Name' and 'Episode_Title' are dropped as they are unlikely to contribute to the prediction of 'Listening_Time_minutes'.\n2. **Handle Missing Values**: Missing values are filled based on domain knowledge (e.g., median for numerical features, mode for categorical features).\n3. **Binary Features**: Created binary features for popular hosts and guests to capture their influence on listening time.\n4. **One-Hot Encoding**: Categorical features are one-hot encoded to make them suitable for machine learning models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\nimport warnings\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:38:50.188163Z","iopub.status.busy":"2025-04-23T08:38:50.18752Z","iopub.status.idle":"2025-04-23T08:38:50.191812Z","shell.execute_reply":"2025-04-23T08:38:50.191147Z","shell.execute_reply.started":"2025-04-23T08:38:50.188143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop unnecessary columns\ncolumns_to_drop = ['Podcast_Name', 'Episode_Title', 'id']\ncolumns_to_drop1 = ['Podcast_Name', 'Episode_Title']\ntrain_df = train_df.drop(columns_to_drop, axis=1)\ntest_df = test_df.drop(columns_to_drop1, axis=1)\n\n# Handle missing values first\n# Fill missing values in Episode_Length_minutes with median by Genre\nlength_medians = train_df.groupby('Genre')['Episode_Length_minutes'].transform('median')\ntrain_df['Episode_Length_minutes'] = train_df['Episode_Length_minutes'].fillna(length_medians)\ntest_df['Episode_Length_minutes'] = test_df.groupby('Genre')['Episode_Length_minutes'].transform(lambda x: x.fillna(x.median()))\n\n# Fill missing values in Guest_Popularity_percentage with 0 (assuming no guest means 0 popularity)\ntrain_df['Guest_Popularity_percentage'] = train_df['Guest_Popularity_percentage'].fillna(0)\ntest_df['Guest_Popularity_percentage'] = test_df['Guest_Popularity_percentage'].fillna(0)\n\n# Fill missing values in Number_of_Ads with mode\ntrain_df['Number_of_Ads'] = train_df['Number_of_Ads'].fillna(train_df['Number_of_Ads'].mode()[0])\ntest_df['Number_of_Ads'] = test_df['Number_of_Ads'].fillna(test_df['Number_of_Ads'].mode()[0])\n\n# Create binary features for popular hosts and guests\ntrain_df['is_popular_host'] = (train_df['Host_Popularity_percentage'] > 70).astype(int)\ntrain_df['is_popular_guest'] = (train_df['Guest_Popularity_percentage'] > 70).astype(int)\n\ntest_df['is_popular_host'] = (test_df['Host_Popularity_percentage'] > 70).astype(int)\ntest_df['is_popular_guest'] = (test_df['Guest_Popularity_percentage'] > 70).astype(int)\n\n# One-hot encode categorical features\ncategorical_features = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\n# Get dummy variables for each categorical feature\nfor feature in categorical_features:\n    # Fit on both train and test to ensure all categories are captured\n    all_categories = pd.concat([train_df[feature], test_df[feature]]).unique()\n    \n    # Create dummy variables\n    train_dummies = pd.get_dummies(train_df[feature], prefix=feature)\n    test_dummies = pd.get_dummies(test_df[feature], prefix=feature)\n    \n    # Add missing columns to test set\n    for col in train_dummies.columns:\n        if col not in test_dummies.columns:\n            test_dummies[col] = 0\n    \n    # Add missing columns to train set\n    for col in test_dummies.columns:\n        if col not in train_dummies.columns:\n            train_dummies[col] = 0\n    \n    # Ensure columns are in the same order\n    test_dummies = test_dummies[train_dummies.columns]\n    \n    # Add to dataframes\n    train_df = pd.concat([train_df, train_dummies], axis=1)\n    test_df = pd.concat([test_df, test_dummies], axis=1)\n    \n    # Drop original categorical column\n    train_df = train_df.drop(feature, axis=1)\n    test_df = test_df.drop(feature, axis=1)\n\n# Scale numerical features\nnumerical_features = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n                     'Guest_Popularity_percentage', 'Number_of_Ads']\n\nscaler = StandardScaler()\ntrain_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\ntest_df[numerical_features] = scaler.transform(test_df[numerical_features])\n\nprint('Final feature set shape:', train_df.shape)\nprint('\\nFeature names:')\nprint(train_df.columns.tolist())","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:38:53.169312Z","iopub.status.busy":"2025-04-23T08:38:53.168999Z","iopub.status.idle":"2025-04-23T08:38:54.171353Z","shell.execute_reply":"2025-04-23T08:38:54.170629Z","shell.execute_reply.started":"2025-04-23T08:38:53.16929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## import librabies","metadata":{}},{"cell_type":"code","source":"# Install required ML packages\n!pip install scikit-learn lightgbm xgboost optuna","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:31:08.163753Z","iopub.status.busy":"2025-04-23T08:31:08.163176Z","iopub.status.idle":"2025-04-23T08:31:11.150185Z","shell.execute_reply":"2025-04-23T08:31:11.14924Z","shell.execute_reply.started":"2025-04-23T08:31:08.163732Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate features and target\nX = train_df.drop(['Listening_Time_minutes'], axis=1)\ny = train_df['Listening_Time_minutes']\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint('Training set shape:', X_train.shape)\nprint('Validation set shape:', X_val.shape)","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:39:00.281329Z","iopub.status.busy":"2025-04-23T08:39:00.280857Z","iopub.status.idle":"2025-04-23T08:39:00.427211Z","shell.execute_reply":"2025-04-23T08:39:00.42637Z","shell.execute_reply.started":"2025-04-23T08:39:00.281305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.execute_input":"2025-04-23T06:25:54.555911Z","iopub.status.busy":"2025-04-23T06:25:54.555576Z","iopub.status.idle":"2025-04-23T06:25:54.578878Z","shell.execute_reply":"2025-04-23T06:25:54.578136Z","shell.execute_reply.started":"2025-04-23T06:25:54.555884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Updated evaluation function to include MAE\nfrom sklearn.metrics import mean_absolute_error\n\ndef evaluate_model(y_true, y_pred, model_name):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f'{model_name} Results:')\n    print(f'RMSE: {rmse:.4f}')\n    print(f'MAE: {mae:.4f}')\n    print(f'R2 Score: {r2:.4f}\\n')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:31:13.902549Z","iopub.status.busy":"2025-04-23T08:31:13.90187Z","iopub.status.idle":"2025-04-23T08:31:13.9065Z","shell.execute_reply":"2025-04-23T08:31:13.905817Z","shell.execute_reply.started":"2025-04-23T08:31:13.902526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models Training and Optimization","metadata":{}},{"cell_type":"markdown","source":"### Implementation of lightGBM \nlightGBM and optuna for params optimization","metadata":{}},{"cell_type":"code","source":"# # LightGBM model with Optuna optimization and GPU support\n# def objective(trial):\n#     params = {\n#         'objective': 'regression',\n#         'metric': 'rmse',\n#         'verbosity': -1,\n#         'boosting_type': 'gbdt',\n#         'device': 'gpu',  # Enable GPU\n#         'gpu_platform_id': 0,\n#         'gpu_device_id': 0,\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n#         'max_depth': trial.suggest_int('max_depth', 3, 12),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n#     }\n    \n#     model = lgb.LGBMRegressor(**params)\n#     scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n#     return -scores.mean()\n\n# # Run Optuna optimization\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=2)\n\n# # Get best parameters\n# best_params = study.best_params\n# best_params.update({\n#     'device': 'gpu',\n#     'gpu_platform_id': 0,\n#     'gpu_device_id': 0\n# })\n# # print('Best parameters:', best_params)","metadata":{"execution":{"execution_failed":"2025-04-23T08:29:38.728Z","iopub.execute_input":"2025-04-23T06:36:21.933409Z","iopub.status.busy":"2025-04-23T06:36:21.933132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best parameters from Optuna optimization (Trial 22)\nbest_params = {\n    'n_estimators': 916,\n    'learning_rate': 0.08366680842136756,\n    'num_leaves': 99,\n    'max_depth': 11,\n    'min_child_samples': 58,\n    'subsample': 0.8939146369158769,\n    'colsample_bytree': 0.9111777868466708,\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'verbosity': -1,\n    'boosting_type': 'gbdt'\n}","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:32:54.270654Z","iopub.status.busy":"2025-04-23T08:32:54.269721Z","iopub.status.idle":"2025-04-23T08:32:54.275007Z","shell.execute_reply":"2025-04-23T08:32:54.274065Z","shell.execute_reply.started":"2025-04-23T08:32:54.270626Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train final model with best parameters\nlgb_model = lgb.LGBMRegressor(**best_params)\nlgb_model.fit(X_train, y_train)\n\n# Make predictions\nlgb_train_pred = lgb_model.predict(X_train)\nlgb_val_pred = lgb_model.predict(X_val)\n\n# Evaluate model\nprint('Training Results:')\nevaluate_model(y_train, lgb_train_pred, 'LightGBM')\nprint('Validation Results:')\nevaluate_model(y_val, lgb_val_pred, 'LightGBM')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:34:43.12163Z","iopub.status.busy":"2025-04-23T08:34:43.121074Z","iopub.status.idle":"2025-04-23T08:35:30.543719Z","shell.execute_reply":"2025-04-23T08:35:30.54296Z","shell.execute_reply.started":"2025-04-23T08:34:43.121604Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Implementation of xgb \nUsing Optuna to find the best hyperparameters for the XGBoost model.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n# Define the objective function for Optuna\ndef xgb_objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n        'random_state': 42,\n        'objective': 'reg:squarederror',  # Standard regression objective\n        'eval_metric': 'rmse'             # Root mean squared error metric\n    }\n    \n    xgb_model = XGBRegressor(**params)\n    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n    return -scores.mean()\n\n# Run Optuna optimization\nxgb_study = optuna.create_study(direction='minimize')\nxgb_study.optimize(xgb_objective, n_trials=30)\n\n# Get the best parameters\nxgb_best_params = xgb_study.best_params\nprint('Best parameters for XGBoost:', xgb_best_params)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train XGBoost Model with Best Parameters\nUsing the best parameters from Optuna to train the XGBoost model.","metadata":{}},{"cell_type":"code","source":"# Train XGBoost model with best parameters\nxgb_model = XGBRegressor(**xgb_best_params)\nxgb_model.fit(X_train, y_train)\n\n# Make predictions\nxgb_train_pred = xgb_model.predict(X_train)\nxgb_val_pred = xgb_model.predict(X_val)\n\n# Evaluate XGBoost model\nprint('XGBoost Training Results:')\nevaluate_model(y_train, xgb_train_pred, 'XGBoost')\nprint('XGBoost Validation Results:')\nevaluate_model(y_val, xgb_val_pred, 'XGBoost')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate both LightGBM and XGBoost models\n# LightGBM Evaluation\nprint('LightGBM Training Results:')\nevaluate_model(y_train, lgb_train_pred, 'LightGBM')\nprint('LightGBM Validation Results:')\nevaluate_model(y_val, lgb_val_pred, 'LightGBM')\n\n# XGBoost Evaluation\nprint('XGBoost Training Results:')\nevaluate_model(y_train, xgb_train_pred, 'XGBoost')\nprint('XGBoost Validation Results:')\nevaluate_model(y_val, xgb_val_pred, 'XGBoost')\n\n# Combined Residuals Analysis\nplt.figure(figsize=(15, 10))\n\n# LightGBM Residuals\nplt.subplot(221)\nsns.histplot(lgb_train_pred - y_train, kde=True)\nplt.title('LightGBM Training Residuals Distribution')\nplt.xlabel('Residuals')\n\nplt.subplot(222)\nsns.histplot(lgb_val_pred - y_val, kde=True)\nplt.title('LightGBM Validation Residuals Distribution')\nplt.xlabel('Residuals')\n\n# XGBoost Residuals\nplt.subplot(223)\nsns.histplot(xgb_train_pred - y_train, kde=True)\nplt.title('XGBoost Training Residuals Distribution')\nplt.xlabel('Residuals')\n\nplt.subplot(224)\nsns.histplot(xgb_val_pred - y_val, kde=True)\nplt.title('XGBoost Validation Residuals Distribution')\nplt.xlabel('Residuals')\n\nplt.tight_layout()\nplt.show()\n\n# Combined Scatter Plots\nplt.figure(figsize=(15, 10))\n\n# LightGBM Scatter Plots\nplt.subplot(221)\nplt.scatter(lgb_train_pred, y_train, alpha=0.5)\nplt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\nplt.title('LightGBM Training: Predicted vs Actual')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\n\nplt.subplot(222)\nplt.scatter(lgb_val_pred, y_val, alpha=0.5)\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nplt.title('LightGBM Validation: Predicted vs Actual')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\n\n# XGBoost Scatter Plots\nplt.subplot(223)\nplt.scatter(xgb_train_pred, y_train, alpha=0.5)\nplt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\nplt.title('XGBoost Training: Predicted vs Actual')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\n\nplt.subplot(224)\nplt.scatter(xgb_val_pred, y_val, alpha=0.5)\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nplt.title('XGBoost Validation: Predicted vs Actual')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make Predictions on Test Set","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:43:33.671529Z","iopub.status.busy":"2025-04-23T08:43:33.67071Z","iopub.status.idle":"2025-04-23T08:43:33.675835Z","shell.execute_reply":"2025-04-23T08:43:33.67534Z","shell.execute_reply.started":"2025-04-23T08:43:33.671505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### test for lgb","metadata":{}},{"cell_type":"code","source":"# Prepare test data\ntest_features = test_df.drop(['id'], axis=1)\n\n# Make predictions\ntest_predictions = lgb_model.predict(test_features)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'Listening_Time_minutes': test_predictions\n})\n\nsubmission.to_csv('lgb_submission.csv', index=False)\nprint('lightGBM Submission file created!')","metadata":{"execution":{"iopub.execute_input":"2025-04-23T08:43:35.972446Z","iopub.status.busy":"2025-04-23T08:43:35.971898Z","iopub.status.idle":"2025-04-23T08:43:46.70682Z","shell.execute_reply":"2025-04-23T08:43:46.706041Z","shell.execute_reply.started":"2025-04-23T08:43:35.972422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### test for xgb","metadata":{}},{"cell_type":"code","source":"# Make predictions on test set for XGBoost\nxgb_test_predictions = xgb_model.predict(test_features)\n\n# Create submission file for XGBoost\nxgb_submission = pd.DataFrame({\n    'id': test_df['id'],\n    'Listening_Time_minutes': xgb_test_predictions\n})\n\nxgb_submission.to_csv('xgb_submission.csv', index=False)\nprint('XGBoost submission file created!')","metadata":{},"outputs":[],"execution_count":null}]}