{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/llkh0a/boosting-regression-models?scriptVersionId=244410351\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üß† Introduction\nThis notebook evaluates the following machine learning models to predict calorie expenditure:\n\n1. üìà Linear Regression\n2. üìâ Ridge Regression\n3. üßÆ Lasso Regression\n4. üîó Elastic Net\n5. üå≤ Random Forest Regressor\n6. üöÄ XGBoost Regressor\n7. üí° LightGBM Regressor\n8. üê± CatBoost Regressor\n\nAdditionally, hyperparameter tuning is performed using Optuna for XGBoost, LightGBM, and CatBoost.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\n\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s5e5/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:30:05.752314Z","iopub.execute_input":"2025-05-07T09:30:05.752536Z","iopub.status.idle":"2025-05-07T09:30:11.690609Z","shell.execute_reply.started":"2025-05-07T09:30:05.75251Z","shell.execute_reply":"2025-05-07T09:30:11.689719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train Shape:\", train.shape)\nprint(\"Test Shape:\", test.shape)\nprint(\"\\nTrain Info:\")\ntrain.info()\nprint(\"\\nTest Info:\")\ntest.info()\nprint(\"\\nTrain Describe:\")\ntrain.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:11.346866Z","iopub.execute_input":"2025-05-07T09:35:11.347165Z","iopub.status.idle":"2025-05-07T09:35:11.716549Z","shell.execute_reply.started":"2025-05-07T09:35:11.347143Z","shell.execute_reply":"2025-05-07T09:35:11.715771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(train['Calories'], bins=50, kde=True)\nplt.title('Distribution of Calories')\nplt.xlabel('Calories')\nplt.ylabel('Count')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(np.log1p(train['Calories']), bins=50, kde=True)\nplt.title('Distribution of Log(Calories + 1)')\nplt.xlabel('Log(Calories + 1)')\nplt.ylabel('Count')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:30:12.102178Z","iopub.execute_input":"2025-05-07T09:30:12.102406Z","iopub.status.idle":"2025-05-07T09:30:12.106515Z","shell.execute_reply.started":"2025-05-07T09:30:12.102387Z","shell.execute_reply":"2025-05-07T09:30:12.10565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(2, 3, i)\n    sns.histplot(train[feature], bins=30, kde=True)\n    plt.title(f'Distribution of {feature}')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSex Distribution:\")\nprint(train['Sex'].value_counts())\n\nplt.figure(figsize=(6, 4))\nsns.countplot(x='Sex', data=train)\nplt.title('Distribution of Sex')\nplt.show()\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='Sex', y='Calories', data=train)\nplt.title('Calories by Sex')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['Sex_encoded'] = le.fit_transform(train['Sex'])\n\ncorr = train[numerical_features + ['Calories', 'Sex_encoded']].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:22.927152Z","iopub.execute_input":"2025-05-07T09:35:22.92745Z","iopub.status.idle":"2025-05-07T09:35:23.755132Z","shell.execute_reply.started":"2025-05-07T09:35:22.927429Z","shell.execute_reply":"2025-05-07T09:35:23.754107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ü§ñ Declare models and training progress\nIn this section, we will implement and evaluate various machine learning models to predict calorie expenditure.","metadata":{}},{"cell_type":"markdown","source":"## models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# Prepare data\nX = train[['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Sex_encoded']]\ny = train['Calories']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:34.574Z","iopub.execute_input":"2025-05-07T09:35:34.574291Z","iopub.status.idle":"2025-05-07T09:35:34.740507Z","shell.execute_reply.started":"2025-05-07T09:35:34.574273Z","shell.execute_reply":"2025-05-07T09:35:34.739559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define models\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(),\n    'Lasso Regression': Lasso(),\n    'Elastic Net': ElasticNet(),\n    'Random Forest': RandomForestRegressor(random_state=42),\n    'XGBoost': XGBRegressor(random_state=42),\n    'LightGBM': LGBMRegressor(random_state=42),\n    'CatBoost': CatBoostRegressor(verbose=0, random_state=42)\n}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate models\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test, np.maximum(0, y_pred)))\n    results[name] = rmsle\n\n# Display results\nfor name, rmsle in results.items():\n    print(f'{name}: RMSLE = {rmsle:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = min(results, key=results.get)\nprint(f'\\nBest Model: {best_model} with RMSLE = {results[best_model]:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üì§ Submission","metadata":{}},{"cell_type":"code","source":"test['Sex_encoded'] = le.transform(test['Sex'])\nids = test['id']\ntest.drop(columns=['id'])\ntest = test[['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp','Sex_encoded']]\n#submission for each model\nfor name, model in models.items():\n    y_pred = np.clip(model.predict(test), a_min=0, a_max=None)\n    submission_df = pd.DataFrame({'id': ids, 'Calories': y_pred})\n    submission_df.to_csv(f'submission_{name}.csv', index=False)\n    print(f'Submission file for {name} created: submission_{name}.csv')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üéØ Hyperparameter Tuning with Optuna\nIn this section, we will use Optuna to optimize the hyperparameters of XGBoost, LightGBM, and CatBoost models.","metadata":{}},{"cell_type":"markdown","source":"## tuning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:17.283961Z","iopub.execute_input":"2025-05-07T09:31:17.284279Z","iopub.status.idle":"2025-05-07T09:31:17.773536Z","shell.execute_reply.started":"2025-05-07T09:31:17.28425Z","shell.execute_reply":"2025-05-07T09:31:17.772659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define objective function for XGBoost\ndef objective_xgb(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n    }\n    model = XGBRegressor(**params, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test, np.maximum(0, y_pred)))\n    return rmsle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:19.706096Z","iopub.execute_input":"2025-05-07T09:31:19.706415Z","iopub.status.idle":"2025-05-07T09:31:19.712732Z","shell.execute_reply.started":"2025-05-07T09:31:19.706389Z","shell.execute_reply":"2025-05-07T09:31:19.71183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define objective function for LightGBM\ndef objective_lgbm(trial):\n# Adjust LightGBM hyperparameters\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', -1, 10),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10)\n    }\n    model = LGBMRegressor(**params, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test, np.maximum(0, y_pred)))\n    return rmsle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:29.513452Z","iopub.execute_input":"2025-05-07T09:31:29.513802Z","iopub.status.idle":"2025-05-07T09:31:29.520408Z","shell.execute_reply.started":"2025-05-07T09:31:29.513777Z","shell.execute_reply":"2025-05-07T09:31:29.51939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define objective function for CatBoost\ndef objective_catboost(trial):\n    params = {\n        'iterations': trial.suggest_int('iterations', 100, 1000),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0)\n    }\n    model = CatBoostRegressor(**params, verbose=0, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test, np.maximum(0, y_pred)))\n    return rmsle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:31.020407Z","iopub.execute_input":"2025-05-07T09:31:31.020744Z","iopub.status.idle":"2025-05-07T09:31:31.026248Z","shell.execute_reply.started":"2025-05-07T09:31:31.02072Z","shell.execute_reply":"2025-05-07T09:31:31.025521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run Optuna studies\n# study_xgb = optuna.create_study(direction='minimize')\n# study_xgb.optimize(objective_xgb, n_trials=20)\n# print('Best parameters for XGBoost:', study_xgb.best_params)\n# print('Best RMSLE for XGBoost:', study_xgb.best_value)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# study_lgbm = optuna.create_study(direction='minimize')\n# study_lgbm.optimize(objective_lgbm, n_trials=20)\n# print('Best parameters for LightGBM:', study_lgbm.best_params)\n# print('Best RMSLE for LightGBM:', study_lgbm.best_value)\n","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# study_catboost = optuna.create_study(direction='minimize')\n# study_catboost.optimize(objective_catboost, n_trials=20)\n# print('Best parameters for CatBoost:', study_catboost.best_params)\n# print('Best RMSLE for CatBoost:', study_catboost.best_value)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## best params \nfor convenient, you might consider copy those params instead","metadata":{}},{"cell_type":"code","source":"# print('Best parameters for CatBoost:', study_catboost.best_params)\n# print('Best parameters for LightGBM:', study_lgbm.best_params)\n# print('Best parameters for XGBoost:', study_xgb.best_params)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#using best params from optuna study of the previous version of this notebook\nlgbm_best_params     = {'n_estimators': 861, 'learning_rate': 0.07414834307911929, 'num_leaves': 43, 'min_data_in_leaf': 37, 'max_depth': 10, 'feature_fraction': 0.8873877337635245, 'bagging_fraction': 0.7695289271584665, 'bagging_freq': 3}\nxgb_best_pamras      = {'n_estimators': 976, 'max_depth': 10, 'learning_rate': 0.019958650721817035, 'subsample': 0.6722659271541026, 'colsample_bytree': 0.849966898586394}\ncatboost_best_params = {'iterations': 756, 'depth': 9, 'learning_rate': 0.05626275170216383, 'l2_leaf_reg': 0.008347575844872637}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:43.107052Z","iopub.execute_input":"2025-05-07T09:35:43.107361Z","iopub.status.idle":"2025-05-07T09:35:43.1125Z","shell.execute_reply.started":"2025-05-07T09:35:43.107337Z","shell.execute_reply":"2025-05-07T09:35:43.111762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Training Progress for CatBoost, LightGBM, and XGBoost\nWe will train the models using the best parameters obtained from Optuna and visualize the training progress.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Train CatBoost\ncatboost_params = catboost_best_params\ncatboost_model = CatBoostRegressor(**catboost_params, verbose=100, random_state=42)\ncatboost_model.fit(X_train, y_train, eval_set=(X_test, y_test), plot=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:45.310621Z","iopub.execute_input":"2025-05-07T09:35:45.310926Z","iopub.status.idle":"2025-05-07T09:36:25.448245Z","shell.execute_reply.started":"2025-05-07T09:35:45.310906Z","shell.execute_reply":"2025-05-07T09:36:25.447429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train LightGBM\nlgbm_params = lgbm_best_params\nlgbm_model = LGBMRegressor(**lgbm_params, random_state=42)\nlgbm_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='rmse')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:36:46.322823Z","iopub.execute_input":"2025-05-07T09:36:46.323513Z","iopub.status.idle":"2025-05-07T09:37:13.95446Z","shell.execute_reply.started":"2025-05-07T09:36:46.323464Z","shell.execute_reply":"2025-05-07T09:37:13.953635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Train XGBoost\nxgb_params = xgb_best_pamras\nxgb_model = XGBRegressor(**xgb_params, random_state=42)\nxgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='rmse', verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:37:54.982526Z","iopub.execute_input":"2025-05-07T09:37:54.98286Z","iopub.status.idle":"2025-05-07T09:38:48.991055Z","shell.execute_reply.started":"2025-05-07T09:37:54.982836Z","shell.execute_reply":"2025-05-07T09:38:48.990177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üì§ Submission\nGenerate the final submission files for CatBoost, LightGBM, and XGBoost.","metadata":{}},{"cell_type":"code","source":"# Generate predictions and submission files\nmodels = {\"CatBoost\": catboost_model, \"LightGBM\": lgbm_model, \"XGBoost\": xgb_model}\nfor name, model in models.items():\n    predictions = np.clip(model.predict(test), a_min=0, a_max=None)\n    submission = pd.DataFrame({'id': ids, 'Calories': predictions})\n    submission.to_csv(f'submission_{name} with params tuned.csv', index=False)\n    print(f'Submission file for {name} created: submission_{name}.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:38:54.014182Z","iopub.execute_input":"2025-05-07T09:38:54.01499Z","iopub.status.idle":"2025-05-07T09:38:54.088372Z","shell.execute_reply.started":"2025-05-07T09:38:54.014965Z","shell.execute_reply":"2025-05-07T09:38:54.087207Z"}},"outputs":[],"execution_count":null}]}